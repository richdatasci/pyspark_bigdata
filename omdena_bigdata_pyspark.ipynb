{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "423415a4-057e-40a5-a4d3-273ff0f2da2c",
      "metadata": {
        "id": "423415a4-057e-40a5-a4d3-273ff0f2da2c"
      },
      "source": [
        "# `BIG DATA FUNDAMENTALS WITH PYSPARK`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abe42a9a-b7fd-4b8a-8ce5-fc0584504f65",
      "metadata": {
        "id": "abe42a9a-b7fd-4b8a-8ce5-fc0584504f65"
      },
      "source": [
        "## What is Big Data?\n",
        "Big data is a term used to refer to the study and applications of data sets that are too\n",
        "complex for traditional data-processing software `- wikipedia`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "757f61ed-b7da-4b8a-b183-b23357d31dd9",
      "metadata": {
        "id": "757f61ed-b7da-4b8a-b183-b23357d31dd9"
      },
      "source": [
        "## The 3 V's of Big Data\n",
        "- `Volume:` Variety and Velocity\n",
        "- `Volume:` Size of the data\n",
        "- `Variety:` Different sources & formats\n",
        "- `Velocity:` Speed of teh data "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9265051a-f468-477a-8999-06da73f799ee",
      "metadata": {
        "id": "9265051a-f468-477a-8999-06da73f799ee"
      },
      "source": [
        "## Big Data concepts and Terminology\n",
        "- `Clustered computing:` Collection of resources of multiple machines\n",
        "- `Parallel computing:` Simultaneous computation\n",
        "- `Distributed computing:` Collection of nodes (networked computers) that run in parallel\n",
        "- `Batch processing:` Breaking the job into small pieces and running them on individual\n",
        "machines\n",
        "- `Real-time processing:` Immediate processing of data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fff3ff5-cdef-4b46-a5a1-93f163bf5f76",
      "metadata": {
        "id": "6fff3ff5-cdef-4b46-a5a1-93f163bf5f76"
      },
      "source": [
        "NOTES: **Clustered computing** is the pooling of resources of multiple machines to complete jobs. **Parallel computing** is a type of computation in which many calculations are carried out simultaneously. A **distributed computing** involves nodes or networked computers that run jobs in parallel. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02a73955-9f17-4d52-b22e-37ec37e10d58",
      "metadata": {
        "id": "02a73955-9f17-4d52-b22e-37ec37e10d58"
      },
      "source": [
        "## Big Data processing systems\n",
        "- `Hadoop/MapReduce:` Scalable and fault tolerant framework written in Java\n",
        "    - Open SOurce\n",
        "    - Batch Processing\n",
        "- `Apache Spark:` General purpose and lightning fast cluster computing system\n",
        "    - Open source\n",
        "    - Both batch and real-time data processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa726cb-4ff1-41a9-9053-ee7fc30ba910",
      "metadata": {
        "id": "caa726cb-4ff1-41a9-9053-ee7fc30ba910"
      },
      "source": [
        "## Features of Apache Spark framework\n",
        "- Distributed cluster computing framework\n",
        "- Efficient in-memory computations for large data sets\n",
        "- Lightning fast data processing framework\n",
        "- Provides support for Java, Scala, Python, R and SQ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95ad3a4f-bf4b-4e69-927d-7fa62355130c",
      "metadata": {
        "id": "95ad3a4f-bf4b-4e69-927d-7fa62355130c"
      },
      "source": [
        "## Apache Spark Components\n",
        "![image.png](attachment:0cb072c7-7396-4fa2-a300-31ce5a941d0e.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db586398-1542-46e7-af82-b727bdc180c7",
      "metadata": {
        "id": "db586398-1542-46e7-af82-b727bdc180c7"
      },
      "source": [
        "## Spark modes of deployment\n",
        "- `Local mode:` Single machine such as your laptop\n",
        "    - Local model convenient for testing, debugging and demonstration\n",
        "- `Cluster mode:` Set of pre-defined machines\n",
        "    - Good for production\n",
        "- Workfkow: Local -> Cluster\n",
        "- No Code Changes Necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "569d2d43-41e9-475a-a130-f8f69c554ea5",
      "metadata": {
        "id": "569d2d43-41e9-475a-a130-f8f69c554ea5"
      },
      "source": [
        "# `PySpark: Spark with Python`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bc3eb80-5b25-444d-a713-f91fb7b30ff3",
      "metadata": {
        "id": "0bc3eb80-5b25-444d-a713-f91fb7b30ff3"
      },
      "source": [
        "## Overview of PySpark\n",
        "- Apache Spark is written in Scala\n",
        "- To support Python with Spark, Apache Spark Community released PySpark\n",
        "- Similar computation speed and power as Scala\n",
        "- PySpark APIs are similar to Pandas and Scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91233950-9b0f-4f9f-be82-4a505aef723a",
      "metadata": {
        "id": "91233950-9b0f-4f9f-be82-4a505aef723a"
      },
      "source": [
        "## What is Spark shell?\n",
        "- Interactive environment for running Spark jobs\n",
        "- Helpful for fast interactive prototyping\n",
        "- Spark’s shells allow interacting with data on disk or in memory\n",
        "- Three different Spark shells:\n",
        "    - Spark-shell for Scala\n",
        "    - PySpark-shell for Python\n",
        "    - SparkR for R"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3761d39a-1c6b-4ecc-9193-2823fa578253",
      "metadata": {
        "id": "3761d39a-1c6b-4ecc-9193-2823fa578253"
      },
      "source": [
        "## PySpark shell\n",
        "- PySpark shell is the Python-based command line tool\n",
        "- PySpark shell allows data scientists interface with Spark data structures\n",
        "- PySpark shell support connecting to a cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b222ed77-60f7-4b30-b0df-95f54db5c873",
      "metadata": {
        "id": "b222ed77-60f7-4b30-b0df-95f54db5c873"
      },
      "source": [
        "## Understanding `SparkContext`\n",
        "- `SparkContext` is an entry point into the world of Spark\n",
        "- An entry point is a way of connecting to Spark cluster\n",
        "- An entry point is like a key to the house\n",
        "- PySpark has a default `SparkContext` called **sc**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecf02b85-763b-463b-88f7-63542b810601",
      "metadata": {
        "id": "ecf02b85-763b-463b-88f7-63542b810601"
      },
      "source": [
        "NOTES:\n",
        "- An **entry point** is where control is transferred from the Operating system to the provided program. \n",
        "- In simpler terms, it's like a key to your house. Without the key you cannot enter the house, similarly, without an entry point, you cannot run any PySpark jobs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88341c11-a5be-4b7c-b516-d7aa64e789ad",
      "metadata": {
        "id": "88341c11-a5be-4b7c-b516-d7aa64e789ad"
      },
      "source": [
        "### Inspecting SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ccf01ad6-dcd3-40eb-a890-b37199781d2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccf01ad6-dcd3-40eb-a890-b37199781d2d",
        "outputId": "3b0b6e23-7e39-4c7f-ae64-ce9022d71d69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 45 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 62.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=17f69c0b6db05f4b5b88060230e252e5196762ec392813fd7c07467f26f6e996\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ],
      "source": [
        "#Verifying Spark \n",
        "!pip install pyspark "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8aafbf3b-0e58-4576-aa0f-792845f58b23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aafbf3b-0e58-4576-aa0f-792845f58b23",
        "outputId": "aa73d864-1247-431c-b4d4-6ec8ab0cdacd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "# finding Pyspark \n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "46171c1f-36d4-425b-afed-b75193df4ef9",
      "metadata": {
        "id": "46171c1f-36d4-425b-afed-b75193df4ef9"
      },
      "outputs": [],
      "source": [
        "#importing Libraries\n",
        "from pyspark import SparkConf\n",
        "from pyspark.context import SparkContext\n",
        "\n",
        "#creating SparkContext\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed959090-8fc8-4466-a457-c987ffbcbe7c",
      "metadata": {
        "id": "ed959090-8fc8-4466-a457-c987ffbcbe7c"
      },
      "source": [
        "`Version:` To retrieve SparkContext version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a2a0aa46-5cdd-4c6b-b81d-170df90d27d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a2a0aa46-5cdd-4c6b-b81d-170df90d27d2",
        "outputId": "638ffebf-a1b7-47ac-a799-a54b052e7dde"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.3.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "sc.version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83c4e166-8cbc-489b-bd0e-2130ffbab886",
      "metadata": {
        "id": "83c4e166-8cbc-489b-bd0e-2130ffbab886"
      },
      "source": [
        "`Python Version:` To retrieve Python version of SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "520d0960-72ae-4095-acf2-aabdcea6e585",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "520d0960-72ae-4095-acf2-aabdcea6e585",
        "outputId": "739cc50d-a514-4de8-bb8b-c84b37782e83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.7'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "sc.pythonVer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9da0a9a-0ad7-48fc-a9c5-6df2940f46ed",
      "metadata": {
        "id": "b9da0a9a-0ad7-48fc-a9c5-6df2940f46ed"
      },
      "source": [
        "`Master:` URL of the cluster or “local” string to run in local mode of SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4435e02f-a97b-4108-bb0d-43de824f1c0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4435e02f-a97b-4108-bb0d-43de824f1c0f",
        "outputId": "89698a25-bf1c-4b1c-cd72-9d7a383dc2c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'local[*]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "sc.master"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b06f85a-94eb-4b25-bfae-bc85defa2a13",
      "metadata": {
        "tags": [],
        "id": "6b06f85a-94eb-4b25-bfae-bc85defa2a13"
      },
      "source": [
        "### Loading data in PySpark\n",
        "\n",
        "- SparkContext's `parallelize()` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1a91ab0b-0c39-4095-a890-51073e5357d2",
      "metadata": {
        "id": "1a91ab0b-0c39-4095-a890-51073e5357d2"
      },
      "outputs": [],
      "source": [
        "rdd = sc.parallelize([1,2,3,4,5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a06f786-ef6c-49ee-b5a5-f49f4706ab47",
      "metadata": {
        "id": "5a06f786-ef6c-49ee-b5a5-f49f4706ab47"
      },
      "source": [
        "- SparkContext's `textFile()` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1b49ae5d-76b0-4b0d-a3d9-21fd5f4286c5",
      "metadata": {
        "id": "1b49ae5d-76b0-4b0d-a3d9-21fd5f4286c5"
      },
      "outputs": [],
      "source": [
        "rdd2 = sc.textFile(\"test.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10cd9250-0631-4ede-8f16-f20a631cfcf7",
      "metadata": {
        "id": "10cd9250-0631-4ede-8f16-f20a631cfcf7"
      },
      "source": [
        "## Use of function in python - `lambda()`, `map()`, `filter()`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52cbcfcb-e5a3-476d-a84a-8e77746e7568",
      "metadata": {
        "id": "52cbcfcb-e5a3-476d-a84a-8e77746e7568"
      },
      "source": [
        "### What are anonymous functions in Python?\n",
        "- Lambda functions are anonymous functions in Python\n",
        "- Very powerful and used in Python. Quite effiencetly with `map()` and `filter()`\n",
        "- Lambda functions create functions to be called later similar to def\n",
        "- It returns the functions without any name (i.e anonymous)\n",
        "- Inline a function definition or to defer execution of a code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "552d06f0-cd4f-4ce8-8382-fd13c4ff1b3d",
      "metadata": {
        "id": "552d06f0-cd4f-4ce8-8382-fd13c4ff1b3d"
      },
      "source": [
        "### Lambda function syntax"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d722974-f44b-4f7b-b022-d234f8145c01",
      "metadata": {
        "id": "0d722974-f44b-4f7b-b022-d234f8145c01"
      },
      "source": [
        "- The general form of lambda functions is\n",
        "\n",
        "`>>> lambda arguments: expression`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "103960cf-c2e1-4958-bcb0-f69a8b7fcc91",
      "metadata": {
        "id": "103960cf-c2e1-4958-bcb0-f69a8b7fcc91"
      },
      "source": [
        "- Example of lambda function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "371b1e8e-e015-4345-97b8-d091cc1db99a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "371b1e8e-e015-4345-97b8-d091cc1db99a",
        "outputId": "0905e360-7be5-40fe-9fc2-12c918e22220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "double = lambda x: x * 2\n",
        "print(double(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5517ced9-3a13-4ca3-b1b8-41bd4dbd0ae5",
      "metadata": {
        "id": "5517ced9-3a13-4ca3-b1b8-41bd4dbd0ae5"
      },
      "source": [
        "### Difference between def vs lambda functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7ca2cc2-4152-44e3-9afc-ec567d7e4d8e",
      "metadata": {
        "id": "f7ca2cc2-4152-44e3-9afc-ec567d7e4d8e"
      },
      "source": [
        "- Python code to illustrate cube of a number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "664432b0-bdaf-4b45-885f-6a9ec57b0f6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "664432b0-bdaf-4b45-885f-6a9ec57b0f6c",
        "outputId": "c72ec970-0757-448f-96de-9e1fed51da78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "#python Funtion\n",
        "def cube(x):\n",
        "    return x ** 3\n",
        "\n",
        "#lambda function\n",
        "g = lambda x: x ** 3\n",
        "\n",
        "#displaying on console\n",
        "print(g(10))\n",
        "print(cube(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4391f11c-cf81-446e-9ee6-f9ed097f2ba9",
      "metadata": {
        "id": "4391f11c-cf81-446e-9ee6-f9ed097f2ba9"
      },
      "source": [
        "### conclusion\n",
        "- No return statement for lambda\n",
        "- Can put lambda function anywhere\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ccdbcb1-bc8a-4458-9941-65810fc48bcf",
      "metadata": {
        "id": "9ccdbcb1-bc8a-4458-9941-65810fc48bcf"
      },
      "source": [
        "### Use of Lambda function in Python - `map()`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58a74c35-e795-4e0a-a7f4-a7faaf613cf2",
      "metadata": {
        "id": "58a74c35-e795-4e0a-a7f4-a7faaf613cf2"
      },
      "source": [
        "- `map()` function takes a function and a list and returns a new list which contains items\n",
        "returned by that function for each item\n",
        "- General syntax of `map()`\n",
        "\n",
        "`>>> map(function, list)`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5943baa4-4103-437a-8484-b8941bc94c97",
      "metadata": {
        "id": "5943baa4-4103-437a-8484-b8941bc94c97"
      },
      "source": [
        "- Example of `map()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "dbbb7d7e-0ec8-43f2-999b-84b81737fb7f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbbb7d7e-0ec8-43f2-999b-84b81737fb7f",
        "outputId": "7126d18a-5878-4d15-f349-14eb9d70bfc8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 4, 5, 6]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "items = [1, 2, 3, 4]\n",
        "list(map(lambda x: x + 2 , items))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bac2700-ec34-44c7-abf3-06cbd56bf447",
      "metadata": {
        "id": "8bac2700-ec34-44c7-abf3-06cbd56bf447"
      },
      "source": [
        "### Use of Lambda function in python - `filter()`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04ed2e62-dabb-4680-9dde-7d4a63253b49",
      "metadata": {
        "id": "04ed2e62-dabb-4680-9dde-7d4a63253b49"
      },
      "source": [
        "- `filter()` function takes a function and a list  and returns  a new list for which the function evaluates as `True`\n",
        "- General Syntax of `filter()`\n",
        "\n",
        "`>>> filter(function, list)`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60d175a6-6aa7-48c6-beac-c3d3021ad1d6",
      "metadata": {
        "id": "60d175a6-6aa7-48c6-beac-c3d3021ad1d6"
      },
      "source": [
        "- Example of `filter()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0e8c15ef-4e5b-4ef8-b652-55fec50226f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e8c15ef-4e5b-4ef8-b652-55fec50226f0",
        "outputId": "09b1b541-ae2f-445d-ca97-700b6963788b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "items = [1, 2, 3, 4]\n",
        "list(filter(lambda x: (x%2 != 0), items))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ed84e9a-85c2-471a-8ff1-e7579a58db2c",
      "metadata": {
        "id": "7ed84e9a-85c2-471a-8ff1-e7579a58db2c"
      },
      "source": [
        "# EXERCISE:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56f27465-7590-4974-80f7-30073b94c742",
      "metadata": {
        "id": "56f27465-7590-4974-80f7-30073b94c742"
      },
      "source": [
        "- Print the version of SparkContext in the PySpark shell.\n",
        "- Print the Python version of SparkContext in the PySpark shell.\n",
        "- What is the master of SparkContext in the PySpark shell?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c362f8af-8c16-4f2d-b3f3-4030409337ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c362f8af-8c16-4f2d-b3f3-4030409337ee",
        "outputId": "ad80368d-45ea-485b-bcf8-5270c9df6f29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkContext Version: 3.3.0\n",
            "Python version of Spark Context in PySpark shell: 3.7\n",
            "Master of Spark Context in  PySpark shell: local[*]\n"
          ]
        }
      ],
      "source": [
        "# Print the version of SparkContext\n",
        "print(\"SparkContext Version:\", sc.version)\n",
        "\n",
        "# Print the Python version of SparkContext\n",
        "print(\"Python version of Spark Context in PySpark shell:\", sc.pythonVer)\n",
        "\n",
        "# Print the master of SparkContext\n",
        "print(\"Master of Spark Context in  PySpark shell:\", sc.master)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7adc4b29-7ac4-4e97-8c85-a00d749303c6",
      "metadata": {
        "id": "7adc4b29-7ac4-4e97-8c85-a00d749303c6"
      },
      "source": [
        "- Create a Python list named numb containing the numbers 1 to 100.\n",
        "- Load the list into Spark using Spark Context's parallelize method and assign it to a variable spark_data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "eb6fabaf-7d6b-4275-afc3-d20ca442d557",
      "metadata": {
        "id": "eb6fabaf-7d6b-4275-afc3-d20ca442d557"
      },
      "outputs": [],
      "source": [
        "# Create a Python list of numbers from 1 to 100 \n",
        "numb = range(1, 101)\n",
        "\n",
        "# Load the list into PySpark  \n",
        "spark_data = sc.parallelize(numb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60da9f1b-9287-49d3-a580-f12ac489f3ca",
      "metadata": {
        "id": "60da9f1b-9287-49d3-a580-f12ac489f3ca"
      },
      "source": [
        "- Load a local text file README.md in PySpark shell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0b9df3a7-4560-403d-822b-fc2fffa37c4c",
      "metadata": {
        "id": "0b9df3a7-4560-403d-822b-fc2fffa37c4c"
      },
      "outputs": [],
      "source": [
        "# Load a local file into PySpark shell\n",
        "lines = sc.textFile(\"text.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05d5d9c2-e2ae-4834-a5a4-6023e623d533",
      "metadata": {
        "id": "05d5d9c2-e2ae-4834-a5a4-6023e623d533"
      },
      "source": [
        "- Print my_list which is available in your environment.\n",
        "- Square each item in my_list using map() and lambda().\n",
        "- Print the result of map function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d31b8530-5a90-4510-95d5-063b0c41c5a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d31b8530-5a90-4510-95d5-063b0c41c5a9",
        "outputId": "4fcabd04-9a8a-4e95-9f7b-a622acb8bd4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input list: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "Squared numbers: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
          ]
        }
      ],
      "source": [
        "# list contaning 1 to 10 numbers\n",
        "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "\n",
        "# Print my_list in the console\n",
        "print(\"Input list:\", my_list)\n",
        "\n",
        "# Square all numbers in my_list\n",
        "squared_list_lambda = list(map(lambda x: x**2 , my_list))\n",
        "\n",
        "# Print the result of the map function\n",
        "print(\"Squared numbers:\", squared_list_lambda)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d99ddf05-666e-4e87-8872-c6e016e0291e",
      "metadata": {
        "id": "d99ddf05-666e-4e87-8872-c6e016e0291e"
      },
      "source": [
        "- Print my_list2 which is available in your environment.\n",
        "- Filter the numbers divisible by 10 from my_list2 using filter() and lambda().\n",
        "- Print the numbers divisible by 10 from my_list2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "816a8294-98a0-410b-9a18-6b58bde0f9f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "816a8294-98a0-410b-9a18-6b58bde0f9f0",
        "outputId": "720b058c-2a67-48ef-cf75-fa6f55195690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input list:: [10, 21, 31, 40, 51, 60, 72, 80, 93, 101]\n",
            "Numbers divisible by 10: [10, 40, 60, 80]\n"
          ]
        }
      ],
      "source": [
        "# mylist2 defined \n",
        "my_list2 = [10, 21, 31, 40, 51, 60, 72, 80, 93, 101]\n",
        "\n",
        "# Print my_list2 in the console\n",
        "print(\"Input list::\", my_list2)\n",
        "\n",
        "# Filter numbers divisible by 10\n",
        "filtered_list = list(filter(lambda x: (x%10 == 0), my_list2))\n",
        "\n",
        "# Print the numbers divisible by 10\n",
        "print(\"Numbers divisible by 10:\", filtered_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b86d5a0-9de9-434a-ac5c-124a83389983",
      "metadata": {
        "id": "0b86d5a0-9de9-434a-ac5c-124a83389983"
      },
      "source": [
        "## Good Luck :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c442282-50af-4265-9ff2-5c74f9b36dff",
      "metadata": {
        "id": "4c442282-50af-4265-9ff2-5c74f9b36dff"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "omdena_bigdata_pyspark.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}